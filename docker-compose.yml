version: '3.8'

services:
  # Reverse Proxy & Load Balancer
  traefik:
    image: traefik:v3.0
    container_name: traefik-gateway
    ports:
      - "8080:8080"  # Back to 8080 for port forwarding
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/traefik:/etc/traefik
      - traefik-certs:/certs
    networks:
      - legal-ai-network
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.dashboard.rule=PathPrefix(`/dashboard`)"
      - "traefik.http.routers.dashboard.service=api@internal"
      - "traefik.http.routers.dashboard.middlewares=dashboard-stripprefix"
      - "traefik.http.middlewares.dashboard-stripprefix.stripprefix.prefixes=/dashboard"
      - "logging.service=traefik"

  # Authentication Service
  auth-service:
    build: ./docker/auth-service
    container_name: auth-service
    ports:
      - "8001:8000"
    environment:
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-your_super_secret_jwt_key_change_this_in_production}
      - JWT_ALGORITHM=HS256
      - JWT_EXPIRATION_HOURS=8
      - LOG_LEVEL=INFO
      - CORS_ORIGINS=["http://localhost", "http://localhost:8501"]
    volumes:
      - ./logs/auth:/app/logs
    networks:
      - legal-ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.auth.rule=Host(`localhost`) && PathPrefix(`/auth`)"
      - "traefik.http.services.auth.loadbalancer.server.port=8000"
      - "logging.service=auth-service"

  # Database/Document Processing Service (FastAPI + Weaviate Client)
  knowledge-service:
    build: ./docker/knowledge-service
    container_name: knowledge-service
    ports:
      - "8000:8000"
    environment:
      - WEAVIATE_URL=http://weaviate-db:8080
      - BGE_EMBEDDING_URL=http://t2v-transformers:8080
      - OLLAMA_URL=http://ollama-service:11434
      - MODEL_MODE=${MODEL_MODE:-dev}
      - OLLAMA_MODEL_DEV=${OLLAMA_MODEL_DEV:-llama3:8b}
      - OLLAMA_MODEL_PROD=${OLLAMA_MODEL_PROD:-llama3:70b}
      - OLLAMA_MODEL=${OLLAMA_MODEL_DEV:-llama3:8b}
      - LOG_LEVEL=INFO
      - API_TITLE=Knowledge Service API
      - API_VERSION=1.0.0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    volumes:
      - ./knowledge-service-data:/app/data
      - ./logs/knowledge-service:/app/logs
    networks:
      - legal-ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - weaviate-db
      - t2v-transformers
      - redis
    labels:
      - "logging.service=knowledge-service"

  # Streamlit Web Application
  streamlit-app:
    build: ./docker/streamlit
    container_name: streamlit-app
    environment:
      - OLLAMA_URL=http://ollama-service:11434
      - WEAVIATE_URL=http://weaviate-db:8080
      - AUTH_SERVICE_URL=http://auth-service:8000
      - KNOWLEDGE_SERVICE_URL=http://knowledge-service:8000
    volumes:
      - ./data/uploads:/app/uploads
      - ./logs/app:/app/logs
    networks:
      - legal-ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - auth-service
      - ollama-service
      - weaviate-db
      - knowledge-service
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.streamlit.rule=Host(`localhost`)"
      - "traefik.http.services.streamlit.loadbalancer.server.port=8501"
      - "logging.service=streamlit-app"

  # LLM Service (Ollama)
  ollama-service:
    image: ollama/ollama:latest
    container_name: ollama-service
    ports:
      - "11434:11434"
    environment:
      - MODEL_MODE=${MODEL_MODE:-dev}
      - OLLAMA_MODEL_DEV=${OLLAMA_MODEL_DEV:-llama3:8b}
      - OLLAMA_MODEL_PROD=${OLLAMA_MODEL_PROD:-llama3:70b}
    volumes:
      - ollama_data:/root/.ollama
      - ./data/models:/models
      - ./scripts:/scripts
    networks:
      - legal-ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ollama.rule=Host(`localhost`) && PathPrefix(`/ollama`)"
      - "traefik.http.services.ollama.loadbalancer.server.port=11434"
      - "logging.service=ollama-service"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Vector Database (Weaviate)
  weaviate-db:
    image: semitechnologies/weaviate:1.26.1
    container_name: weaviate-db
    restart: unless-stopped
    ports:
      - "8081:8080"  # Changed to 8081 to avoid conflict with Traefik
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers'
      ENABLE_MODULES: 'text2vec-transformers'
      CLUSTER_HOSTNAME: 'node1'
      TRANSFORMERS_INFERENCE_API: 'http://t2v-transformers:8080'
      LOG_LEVEL: 'info'
    volumes:
      - weaviate_data:/var/lib/weaviate
    networks:
      - legal-ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/.well-known/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.weaviate.rule=Host(`localhost`) && PathPrefix(`/weaviate`)"
      - "traefik.http.services.weaviate.loadbalancer.server.port=8080"
      - "logging.service=weaviate-db"

  # Text-to-Vector Embeddings (BGE Model)
  t2v-transformers:
    image: semitechnologies/transformers-inference:sentence-transformers-all-MiniLM-L6-v2
    container_name: t2v-transformers
    environment:
      ENABLE_CUDA: '1'
    volumes:
      - transformers_cache:/tmp/.cache
    networks:
      - legal-ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/.well-known/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "logging.service=t2v-transformers"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Document Processing Workflows
  n8n-automation:
    image: n8nio/n8n:latest
    container_name: n8n-automation
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD:-change_this_password}
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://localhost:5678/
      - KNOWLEDGE_SERVICE_URL=http://knowledge-service:8000
    volumes:
      - n8n_data:/home/node/.n8n
      - ./data/uploads:/data/uploads:ro
      - ./logs/n8n:/logs
    networks:
      - legal-ai-network
    restart: unless-stopped
    depends_on:
      - knowledge-service
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.n8n.rule=Host(`localhost`) && PathPrefix(`/n8n`)"
      - "traefik.http.services.n8n.loadbalancer.server.port=5678"
      - "logging.service=n8n-automation"

  # ELK Stack and Beats
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - xpack.security.enabled=false
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
      - ./config/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - legal-ai-network
    restart: unless-stopped
    labels:
      - "logging.service=elasticsearch"

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.1
    container_name: logstash
    volumes:
      - ./config/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./config/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml
    ports:
      - "5044:5044"
      - "9600:9600"
    networks:
      - legal-ai-network
    restart: unless-stopped
    depends_on:
      - elasticsearch
    labels:
      - "logging.service=logstash"

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.1
    container_name: kibana
    volumes:
      - ./config/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml
    ports:
      - "5601:5601"
    networks:
      - legal-ai-network
    restart: unless-stopped
    depends_on:
      - elasticsearch
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.kibana.rule=PathPrefix(`/kibana`)"
      - "traefik.http.routers.kibana.middlewares=kibana-stripprefix"
      - "traefik.http.middlewares.kibana-stripprefix.stripprefix.prefixes=/kibana"
      - "traefik.http.services.kibana.loadbalancer.server.port=5601"
      - "logging.service=kibana"

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.1
    container_name: filebeat
    user: root
    volumes:
      - ./config/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/var/log/legal-ai:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - legal-ai-network
    restart: unless-stopped
    depends_on:
      - logstash
    command: filebeat -e -strict.perms=false
    labels:
      - "logging.service=filebeat"

  metricbeat:
    image: docker.elastic.co/beats/metricbeat:8.11.1
    container_name: metricbeat
    user: root
    volumes:
      - ./config/metricbeat/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml:ro
      - /proc:/hostfs/proc:ro
      - /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro
      - /:/hostfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - legal-ai-network
    restart: unless-stopped
    depends_on:
      - elasticsearch
    command: metricbeat -e -strict.perms=false
    labels:
      - "logging.service=metricbeat"

  # Redis - Message Broker and Progress Storage
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    networks:
      - private-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Celery Worker - Background Document Processing
  celery-worker:
    build:
      context: ./docker/knowledge-service
      dockerfile: Dockerfile
    container_name: celery-worker
    command: celery -A app.celery_app worker --loglevel=info --concurrency=2
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - WEAVIATE_URL=http://weaviate-db:8080
    volumes:
      - ./docker/knowledge-service:/app
    depends_on:
      - redis
      - weaviate-db
    networks:
      - private-network
    restart: unless-stopped

  # Celery Flower - Task Monitoring (Optional)
  celery-flower:
    build:
      context: ./docker/knowledge-service
      dockerfile: Dockerfile
    container_name: celery-flower
    command: celery -A app.celery_app flower --port=5555
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      - redis
      - celery-worker
    networks:
      - private-network

networks:
  legal-ai-network:
    driver: bridge
  private-network:
    driver: bridge

volumes:
  # Data persistence
  ollama_data:
  weaviate_data:
  transformers_cache:
  auth-data:
  n8n_data:
  elasticsearch-data:

  # SSL certificates
  traefik-certs:
  redis_data: